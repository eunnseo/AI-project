{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2cb77bf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index    0\n",
      "QaA      0\n",
      "QaE      0\n",
      "QbA      0\n",
      "QbE      0\n",
      "        ..\n",
      "wr_09    0\n",
      "wr_10    0\n",
      "wr_11    0\n",
      "wr_12    0\n",
      "wr_13    0\n",
      "Length: 78, dtype: int64\n",
      "education 0 건수는 432, 퍼센트는 1.19 %\n",
      "engnat 0 건수는 63, 퍼센트는 0.17 %\n",
      "hand 0 건수는 135, 퍼센트는 0.37 %\n",
      "urban 0 건수는 258, 퍼센트는 0.71 %\n",
      "['QaE', 'QbE', 'QcE', 'QdE', 'QeE', 'QfE', 'QgE', 'QhE', 'QiE', 'QjE', 'QkE', 'QlE', 'QmE', 'QnE', 'QoE', 'QpE', 'QqE', 'QrE', 'QsE', 'QtE']\n",
      "QaE 0 건수는 0, 퍼센트는 0.00 %\n",
      "QbE 0 건수는 0, 퍼센트는 0.00 %\n",
      "QcE 0 건수는 0, 퍼센트는 0.00 %\n",
      "QdE 0 건수는 0, 퍼센트는 0.00 %\n",
      "QeE 0 건수는 0, 퍼센트는 0.00 %\n",
      "QfE 0 건수는 0, 퍼센트는 0.00 %\n",
      "QgE 0 건수는 0, 퍼센트는 0.00 %\n",
      "QhE 0 건수는 1, 퍼센트는 0.00 %\n",
      "QiE 0 건수는 1, 퍼센트는 0.00 %\n",
      "QjE 0 건수는 1, 퍼센트는 0.00 %\n",
      "QkE 0 건수는 2, 퍼센트는 0.01 %\n",
      "QlE 0 건수는 0, 퍼센트는 0.00 %\n",
      "QmE 0 건수는 0, 퍼센트는 0.00 %\n",
      "QnE 0 건수는 0, 퍼센트는 0.00 %\n",
      "QoE 0 건수는 0, 퍼센트는 0.00 %\n",
      "QpE 0 건수는 1, 퍼센트는 0.00 %\n",
      "QqE 0 건수는 1, 퍼센트는 0.00 %\n",
      "QrE 0 건수는 0, 퍼센트는 0.00 %\n",
      "QsE 0 건수는 0, 퍼센트는 0.00 %\n",
      "QtE 0 건수는 0, 퍼센트는 0.00 %\n",
      "['tp01', 'tp02', 'tp03', 'tp04', 'tp05', 'tp06', 'tp07', 'tp08', 'tp09', 'tp10']\n",
      "tp01 0 건수는 3822, 퍼센트는 10.49 %\n",
      "tp02 0 건수는 4356, 퍼센트는 11.96 %\n",
      "tp03 0 건수는 9348, 퍼센트는 25.66 %\n",
      "tp04 0 건수는 4414, 퍼센트는 12.12 %\n",
      "tp05 0 건수는 12682, 퍼센트는 34.82 %\n",
      "tp06 0 건수는 7148, 퍼센트는 19.62 %\n",
      "tp07 0 건수는 7858, 퍼센트는 21.57 %\n",
      "tp08 0 건수는 2547, 퍼센트는 6.99 %\n",
      "tp09 0 건수는 7027, 퍼센트는 19.29 %\n",
      "tp10 0 건수는 1158, 퍼센트는 3.18 %\n",
      "(35092, 129)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-d23a76c2997b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[0mxgb_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m156\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'feature_names must be unique'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "data_p = pd.read_csv(\"train.csv\")\n",
    "\n",
    "#결손값 처리\n",
    "data_p.isna().head(3)\n",
    "print(data_p.isna().sum())\n",
    "\n",
    "\n",
    "####0값을 검사할 피처명 리스트 객체 설정\n",
    "\n",
    "\n",
    "#무응답 해당 피처 : education, engnat, hand, urban\n",
    "zero_features = ['education', 'engnat', 'hand', 'urban']\n",
    "    \n",
    "#전체 데이터 건수\n",
    "total_count = data_p['education'].count()\n",
    "\n",
    "#피처별로 반복 하면서 데이터 값이 0인 데이터 건수 추출하고, 퍼센트 계산\n",
    "for feature in zero_features:\n",
    "    zero_count = data_p[data_p[feature] == 0][feature].count()\n",
    "    print('{0} 0 건수는 {1}, 퍼센트는 {2:.2f} %'.format(feature,  zero_count, 100* zero_count/total_count))\n",
    "    \n",
    "    \n",
    "Q_E = []\n",
    "\n",
    "\n",
    "#Q_E 시간\n",
    "for i in range(97, 117):\n",
    "    a = 'Q' + chr(i) + 'E'\n",
    "    Q_E.append(a)\n",
    "    \n",
    "print(Q_E)\n",
    "    \n",
    "for feature in Q_E:\n",
    "    zero_count = data_p[data_p[feature] == 0][feature].count()\n",
    "    print('{0} 0 건수는 {1}, 퍼센트는 {2:.2f} %'.format(feature,  zero_count, 100* zero_count/total_count))\n",
    "\n",
    "    \n",
    "    \n",
    "#tp\n",
    "TP = []\n",
    "for i in range(1,10):\n",
    "        a = 'tp0' + str(i)\n",
    "        TP.append(a)\n",
    "TP.append('tp10')\n",
    "print(TP)\n",
    "\n",
    "for feature in TP:\n",
    "    zero_count = data_p[data_p[feature] == 0][feature].count()\n",
    "    print('{0} 0 건수는 {1}, 퍼센트는 {2:.2f} %'.format(feature,  zero_count, 100* zero_count/total_count)) \n",
    "    \n",
    "    \n",
    "    \n",
    "#0값을 평균값으로 대체\n",
    "data_p[zero_features]=data_p[zero_features].replace(0, data_p[zero_features].mean())\n",
    "\n",
    "\n",
    "\n",
    "#시간에 대해 이상치 데이터 인덱스 확인 및 제거\n",
    "\n",
    "def get_outlier(df=None, column=None, weight=1.5):\n",
    "    fraud = df[df['voted']==1][column]\n",
    "    quantile_25 = np.percentile(fraud.values, 25)\n",
    "    quantile_75 = np.percentile(fraud.values, 75)\n",
    "    \n",
    "    iqr = quantile_75 - quantile_25\n",
    "    iqr_weight = iqr * weight\n",
    "    lowest_val = quantile_25 - iqr_weight\n",
    "    highest_val = quantile_75 + iqr_weight\n",
    "    \n",
    "    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index\n",
    "    \n",
    "    return outlier_index\n",
    "\n",
    "for feature in Q_E:\n",
    "    outlier_index = get_outlier(df=data_p, column=feature, weight=1.5)\n",
    "    #print('이상치 데이터 인덱스 :', outlier_index)\n",
    "    \n",
    "data_p.drop(outlier_index, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#데이터 변환\n",
    "#원-핫 인코딩 대상 : gender, married, race, religion, \n",
    "#레이블 인코딩 대상 : age-group, \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "data_p[''] = data_p['gender'] == ''\n",
    "\n",
    "\n",
    "#age_group\n",
    "\n",
    "age_group_=['10s', '20s', '30s', '40s', '50s', '60s', '+70s']\n",
    "\n",
    "data_p['age_group_10s'] = data_p['gender'] == '10s'\n",
    "data_p['age_group_20s'] = data_p['gender'] == '20s'\n",
    "data_p['age_group_30s'] = data_p['gender'] == '30s'\n",
    "data_p['age_group_40s'] = data_p['gender'] == '40s'\n",
    "data_p['age_group_50s'] = data_p['gender'] == '50s'\n",
    "data_p['age_group_60s'] = data_p['gender'] == '60s'\n",
    "data_p['age_group_70s'] = data_p['gender'] == '70s'\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#gender\n",
    "\n",
    "gender_=['Male', 'Female']\n",
    "\n",
    "data_p['gender_Male'] = data_p['gender'] == 'Male'\n",
    "data_p['gender_Female'] = data_p['gender'] == 'Female'\n",
    "\n",
    "#marride\n",
    "\n",
    "marride_=['1', '2', '3', '0']\n",
    "\n",
    "data_p['married_1'] = data_p['married'] == 1\n",
    "data_p['married_2'] = data_p['married'] == 2\n",
    "data_p['married_3'] = data_p['married'] == 3\n",
    "data_p['married_4'] = data_p['married'] == 4\n",
    "    \n",
    "\n",
    "#race\n",
    "\n",
    "race_=['Asian', 'Arab', 'Black', 'Indigenous Australian', 'Native American', 'White', 'Other']\n",
    "\n",
    "data_p['race_Asian'] = data_p['race'] == 'Asian'\n",
    "data_p['race_Arab'] = data_p['race'] == 'Arab'\n",
    "data_p['race_Black'] = data_p['race'] == 'Black'\n",
    "data_p['race_Indigenous Australian'] = data_p['race'] == 'Indigenous Australian'\n",
    "data_p['race_Native American'] = data_p['race'] == 'Native American'\n",
    "data_p['race_White'] = data_p['race'] == 'White'\n",
    "data_p['race_Other'] = data_p['race'] == 'Other'\n",
    "\n",
    "\n",
    "#religion\n",
    "\n",
    "religion_=['Agnostic', 'Atheist', 'Buddhist', 'Christian_Catholic', 'Christian_Mormon', 'Christian_Protestant', 'Christian_Other', 'Hindu', 'Jewish', 'Muslim', 'Sikh', 'Other']\n",
    "\n",
    "data_p['religion_Agnostic'] = data_p['religion'] == 'Agnostic'\n",
    "data_p['religion_Atheist'] = data_p['religion'] == 'Atheist'\n",
    "data_p['religion_Buddhist'] = data_p['religion'] == 'Buddhist'\n",
    "data_p['religion_Christian_Catholic'] = data_p['religion'] == 'Christian_Catholic'\n",
    "data_p['religion_Christian_Mormon'] = data_p['religion'] == 'Christian_Mormon'\n",
    "data_p['religion_Christian_Protestant'] = data_p['religion'] == 'Christian_Protestant'\n",
    "data_p['religion_Christian_Other'] = data_p['religion'] == 'Christian_Other'\n",
    "data_p['religion_Hindu'] = data_p['religion'] == 'Hindu'\n",
    "data_p['religion_Jewish'] = data_p['religion'] == 'Jewish'\n",
    "data_p['religion_Muslim'] = data_p['religion'] == 'Muslim'\n",
    "data_p['religion_Sikh'] = data_p['religion'] == 'Sikh'\n",
    "data_p['religion_Other'] = data_p['religion'] == 'Other'\n",
    "\n",
    "data_p[['married_1','married_2','married_3','married_4','married','religion', 'gender', 'race']].head(30)\n",
    "\n",
    "\n",
    "def dummy_data(data, columns) :\n",
    "    for column in columns:\n",
    "        data = pd.concat([data, pd.get_dummies(data[column], prefix = column)], axis=1)\n",
    "        data = data.drop(column, axis=1)\n",
    "    return data\n",
    "\n",
    "dummy_columns = ['race', 'religion', 'gender', 'married']\n",
    "data_k = dummy_data(data_p, dummy_columns)\n",
    "\n",
    "data_k = data_k.drop('index', axis=1)\n",
    "data_k = data_k.drop('age_group', axis=1)\n",
    "\n",
    "data_k.head(20)\n",
    "\n",
    "\n",
    "'''\n",
    "X_features = data_k.iloc[:,:-1]\n",
    "y_lables = data_k.iloc[:,-1]\n",
    "\n",
    "\n",
    "print(X_features.shape)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_lables, test_size =0.2, random_state=0, stratify=y_lables)\n",
    "\n",
    "xgb_clf = XGBClassifier(n_estimators=500, random_state=156)\n",
    "\n",
    "if len(feature_names) != len(set(feature_names)):\n",
    "    raise ValueError('feature_names must be unique')\n",
    "\n",
    "xgb_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"auc\", eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1], average='macro')\n",
    "\n",
    "print(xgb_roc_score)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76747dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ssu] *",
   "language": "python",
   "name": "conda-env-ssu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
